{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE474/574 - Programming Assignment 1\n",
    "\n",
    "For grading, we will execute the submitted notebook as follows:\n",
    "\n",
    "```shell\n",
    "jupyter nbconvert --to python PA1Script.ipynb\n",
    "python PA1Script.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 - Linear Regression with Direct Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM 1\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('PROBLEM 1')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnOLERegression(Xtrain,ytrain):\n",
    "#    print(Xtrain)\n",
    "#    print(\"------------\")\n",
    "#    A=np.array(Xtrain)\n",
    "   A = Xtrain\n",
    "   B = ytrain\n",
    "#    B=np.array(ytrain)\n",
    "   At =np.transpose(A)\n",
    "   AtA=np.dot(At,A)\n",
    "   AtA_Inverse=np.linalg.inv(AtA)\n",
    "   AtA_InverseAt=np.dot(np.linalg.inv(AtA),At)\n",
    "   w=np.dot(AtA_InverseAt,B)\n",
    "   return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOLERegression(w,Xtest,ytest):\n",
    "#     print(Xtest)\n",
    "#     A=np.array(Xtest)\n",
    "#     print(len(A))\n",
    "#     B=np.array(ytest)\n",
    "    A = Xtest\n",
    "    B = ytest\n",
    "    wt=np.transpose(w)\n",
    "#     Xw =  np.dot(Xtest,w)\n",
    "#     y_XW = np.subtract(ytest,Xw)\n",
    "#     y_XWT = np.transpose(y_XW)\n",
    "#     loss = np.dot(y_XWT,y_XW)\n",
    "#     print(loss)\n",
    "    sum=0\n",
    "#     print(len(A[0]))\n",
    "    for i in range(0,len(A)):\n",
    "#      print(wt.shape)\n",
    "#      print(A[i].shape)\n",
    "     wtxi=np.dot(wt,A[i])\n",
    "#      print(A[i])\n",
    "#      print(wt)\n",
    "#      print(wtxi)\n",
    "     yiwtxi=np.subtract(B[i],wtxi)\n",
    "     yiwtxi2=np.square(yiwtxi)\n",
    "#      print(yiwtxi2)\n",
    "     sum=sum+yiwtxi2[0]\n",
    "    print(sum)\n",
    "    average=sum/len(A)\n",
    "    rmse=math.sqrt(average)\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4622066.136386112\n",
      "529292.791373155\n",
      "RMSE without intercept on train data - 138.20\n",
      "RMSE with intercept on train data - 46.77\n",
      "21355072.311183043\n",
      "741568.0363530915\n",
      "RMSE without intercept on test data - 326.76\n",
      "RMSE with intercept on test data - 60.89\n"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain,Xtest,ytest = pickle.load(open('diabetes.pickle','rb'),encoding='latin1')   \n",
    "# add intercept\n",
    "x1 = np.ones((len(Xtrain),1))\n",
    "x2 = np.ones((len(Xtest),1))\n",
    "\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "w = learnOLERegression(Xtrain,ytrain)\n",
    "w_i = learnOLERegression(Xtrain_i,ytrain)\n",
    "# print(np.size(w_i))\n",
    "# print(w)\n",
    "# print(w_i)\n",
    "rmse = testOLERegression(w,Xtrain,ytrain)\n",
    "\n",
    "rmse_i = testOLERegression(w_i,Xtrain_i,ytrain)\n",
    "print('RMSE without intercept on train data - %.2f'%rmse)\n",
    "print('RMSE with intercept on train data - %.2f'%rmse_i)\n",
    "\n",
    "rmse = testOLERegression(w,Xtest,ytest)\n",
    "rmse_i = testOLERegression(w_i,Xtest_i,ytest)\n",
    "print('RMSE without intercept on test data - %.2f'%rmse)\n",
    "print('RMSE with intercept on test data - %.2f'%rmse_i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 - Linear Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PROBLEM 2')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionObjVal(w, X, y):\n",
    "#     A = X\n",
    "#     B = y\n",
    "#     wt=np.transpose(w)\n",
    "\n",
    "#     sum=0\n",
    "\n",
    "#     for i in range(0,len(A)):\n",
    "\n",
    "#      wtxi=np.dot(wt,A[i])\n",
    "\n",
    "#      yiwtxi=np.subtract(B[i],wtxi)\n",
    "#      yiwtxi2=np.square(yiwtxi)\n",
    "# #      print(yiwtxi2)\n",
    "#      sum=sum+yiwtxi2[0]\n",
    "#     loss=sum/2\n",
    "#     print(loss)\n",
    "    w=np.matrix(w)\n",
    "    w=w.transpose()\n",
    "    A=np.dot(X,w)\n",
    "    B=np.subtract(y,A)\n",
    "#     print(B.shape)\n",
    "    Bt=B.transpose()\n",
    "    e=np.dot(Bt,B)\n",
    "    E=0.5*e\n",
    "#     print(E)\n",
    "    error=E[0,0]\n",
    "#     print(error)\n",
    "    # compute squared error (scalar) with respect\n",
    "    # to w (vector) for the given data X and y      \n",
    "    #\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # error = scalar value\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    #error = 0\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionGradient(w, X, y):\n",
    "    AT = X.transpose()\n",
    "    ATA = np.dot(AT,X)\n",
    "#     print(ATA)\n",
    "#     print('=============')\n",
    "    w=np.matrix(w)\n",
    "    w=w.transpose()\n",
    "    ATAW = np.dot(ATA,w)\n",
    "#     print(ATAW)\n",
    "    ATB = np.dot(AT,y)\n",
    "    deltaJ = np.subtract(ATAW,ATB)\n",
    "#     print(deltaJ[0])\n",
    "    \n",
    "    error_grad = np.squeeze(np.asarray(deltaJ))\n",
    "#     print(error_grad)\n",
    "    # compute gradient of squared error (scalar) with respect\n",
    "    # to w (vector) for the given data X and y   \n",
    "    \n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # gradient = d length vector (not a d x 1 matrix)\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE \n",
    "\n",
    "#     error_grad = np.zeros((X.shape[1],))\n",
    "#     print(error_grad)\n",
    "    return error_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3378109.0\n",
      "[[3378109.]]\n",
      "3341743.6059046136\n",
      "[[3341743.60590461]]\n",
      "3198749.2211522246\n",
      "[[3198749.22115222]]\n",
      "2666246.7482091296\n",
      "[[2666246.74820913]]\n",
      "1167837.9135001833\n",
      "[[1167837.91350018]]\n",
      "686786.9487735697\n",
      "[[686786.94877357]]\n",
      "2131909.180212988\n",
      "[[2131909.18021299]]\n",
      "577036.8906445855\n",
      "[[577036.89064459]]\n",
      "464787.96454575355\n",
      "[[464787.96454575]]\n",
      "413349.5460575714\n",
      "[[413349.54605757]]\n",
      "391469.3431997108\n",
      "[[391469.34319971]]\n",
      "3850760.4754244527\n",
      "[[3850760.47542445]]\n",
      "389949.2239483125\n",
      "[[389949.22394831]]\n",
      "386964.4236351571\n",
      "[[386964.42363516]]\n",
      "376742.03387590044\n",
      "[[376742.0338759]]\n",
      "362488.9130801395\n",
      "[[362488.91308014]]\n",
      "994043.3428083748\n",
      "[[994043.34280838]]\n",
      "361369.26490949275\n",
      "[[361369.26490949]]\n",
      "359168.65793643426\n",
      "[[359168.65793643]]\n",
      "351587.8766771733\n",
      "[[351587.87667717]]\n",
      "340433.429730286\n",
      "[[340433.42973029]]\n",
      "731250.1750265609\n",
      "[[731250.17502656]]\n",
      "339401.0772796839\n",
      "[[339401.07727968]]\n",
      "337384.2784433202\n",
      "[[337384.27844332]]\n",
      "330688.14537493343\n",
      "[[330688.14537493]]\n",
      "323542.2325393247\n",
      "[[323542.23253932]]\n",
      "583289.9561822633\n",
      "[[583289.95618226]]\n",
      "322662.95401243155\n",
      "[[322662.95401243]]\n",
      "320993.23855122866\n",
      "[[320993.23855123]]\n",
      "316442.9199688389\n",
      "[[316442.91996884]]\n",
      "315371.1174771621\n",
      "[[315371.11747716]]\n",
      "627118.6565188221\n",
      "[[627118.65651882]]\n",
      "315204.9814043865\n",
      "[[315204.98140439]]\n",
      "314891.12596577575\n",
      "[[314891.12596578]]\n",
      "314070.49277927505\n",
      "[[314070.49277928]]\n",
      "313099.3726274829\n",
      "[[313099.37262748]]\n",
      "313027.05057609355\n",
      "[[313027.05057609]]\n",
      "312590.18433421676\n",
      "[[312590.18433422]]\n",
      "311828.2039618038\n",
      "[[311828.2039618]]\n",
      "311598.04644131893\n",
      "[[311598.04644132]]\n",
      "309827.913741244\n",
      "[[309827.91374124]]\n",
      "307306.66381364095\n",
      "[[307306.66381364]]\n",
      "445167.04409861565\n",
      "[[445167.04409862]]\n",
      "307178.45622887864\n",
      "[[307178.45622888]]\n",
      "306924.63063291926\n",
      "[[306924.63063292]]\n",
      "306012.40275428334\n",
      "[[306012.40275428]]\n",
      "304012.6833229711\n",
      "[[304012.68332297]]\n",
      "357399.40931998455\n",
      "[[357399.40931998]]\n",
      "303841.6680801042\n",
      "[[303841.6680801]]\n",
      "303507.76061626285\n",
      "[[303507.76061626]]\n",
      "302402.9972958917\n",
      "[[302402.99729589]]\n",
      "301257.13912871137\n",
      "[[301257.13912871]]\n",
      "303272.6610057492\n",
      "[[303272.66100575]]\n",
      "300315.48640711926\n",
      "[[300315.48640712]]\n",
      "298798.60251202824\n",
      "[[298798.60251203]]\n",
      "297967.6034484232\n",
      "[[297967.60344842]]\n",
      "393890.6334279266\n",
      "[[393890.63342793]]\n",
      "297911.7417821437\n",
      "[[297911.74178214]]\n",
      "297800.78100319067\n",
      "[[297800.78100319]]\n",
      "297394.533626023\n",
      "[[297394.53362602]]\n",
      "296371.07593565335\n",
      "[[296371.07593565]]\n",
      "405629.943647445\n",
      "[[405629.94364745]]\n",
      "296349.52801522415\n",
      "[[296349.52801522]]\n",
      "296306.48577628616\n",
      "[[296306.48577629]]\n",
      "296144.0080271182\n",
      "[[296144.00802712]]\n",
      "295649.15633579344\n",
      "[[295649.15633579]]\n",
      "295372.0531064007\n",
      "[[295372.0531064]]\n",
      "295247.712296273\n",
      "[[295247.71229627]]\n",
      "294853.1117065398\n",
      "[[294853.11170654]]\n",
      "293898.97195834416\n",
      "[[293898.97195834]]\n",
      "291964.8505560489\n",
      "[[291964.85055605]]\n",
      "289500.29739259795\n",
      "[[289500.2973926]]\n",
      "288795.3766506399\n",
      "[[288795.37665064]]\n",
      "287383.52888929617\n",
      "[[287383.5288893]]\n",
      "286741.9747481501\n",
      "[[286741.97474815]]\n",
      "298818.89036873175\n",
      "[[298818.89036873]]\n",
      "286476.87357402395\n",
      "[[286476.87357402]]\n",
      "285979.60756113776\n",
      "[[285979.60756114]]\n",
      "284755.31068656745\n",
      "[[284755.31068657]]\n",
      "357849.0268605517\n",
      "[[357849.02686055]]\n",
      "284715.82654842734\n",
      "[[284715.82654843]]\n",
      "284637.6358766307\n",
      "[[284637.63587663]]\n",
      "284356.21893436875\n",
      "[[284356.21893437]]\n",
      "283732.0830841174\n",
      "[[283732.08308412]]\n",
      "292617.7124405907\n",
      "[[292617.71244059]]\n",
      "283667.64895563875\n",
      "[[283667.64895564]]\n",
      "283549.5790160398\n",
      "[[283549.57901604]]\n",
      "283319.0392561876\n",
      "[[283319.03925619]]\n",
      "291657.9655079975\n",
      "[[291657.965508]]\n",
      "283305.33032835956\n",
      "[[283305.33032836]]\n",
      "283278.7525159107\n",
      "[[283278.75251591]]\n",
      "283194.7257014021\n",
      "[[283194.7257014]]\n",
      "283133.27105267934\n",
      "[[283133.27105268]]\n",
      "282820.9755852798\n",
      "[[282820.97558528]]\n",
      "282276.8959420191\n",
      "[[282276.89594202]]\n",
      "281694.252219304\n",
      "[[281694.2522193]]\n",
      "281338.13098224846\n",
      "[[281338.13098225]]\n",
      "280825.82947746455\n",
      "[[280825.82947746]]\n",
      "280713.33940570103\n",
      "[[280713.3394057]]\n",
      "288805.9109513327\n",
      "[[288805.91095133]]\n",
      "280670.77118366293\n",
      "[[280670.77118366]]\n",
      "280586.1321932595\n",
      "[[280586.13219326]]\n",
      "280274.5525939016\n",
      "[[280274.5525939]]\n",
      "279459.855992563\n",
      "[[279459.85599256]]\n",
      "452651.52332252706\n",
      "[[452651.52332253]]\n",
      "279451.3396637385\n",
      "[[279451.33966374]]\n",
      "279434.70972082374\n",
      "[[279434.70972082]]\n",
      "279379.65077538096\n",
      "[[279379.65077538]]\n",
      "279322.22913511255\n",
      "[[279322.22913511]]\n",
      "279118.2744155146\n",
      "[[279118.27441551]]\n",
      "279023.10828405374\n",
      "[[279023.10828405]]\n",
      "278678.80354333005\n",
      "[[278678.80354333]]\n",
      "557357.6070866601\n",
      "Gradient Descent Linear Regression RMSE on train data - 47.99\n",
      "603917.1364403854\n",
      "Gradient Descent Linear Regression RMSE on test data - 54.95\n"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain,Xtest,ytest = pickle.load(open('diabetes.pickle','rb'),encoding='latin1')   \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "args = (Xtrain_i,ytrain)\n",
    "opts = {'maxiter' : 50}    # Preferred value.    \n",
    "w_init = np.zeros((Xtrain_i.shape[1],1))\n",
    "soln = minimize(regressionObjVal, w_init, jac=regressionGradient, args=args,method='CG', options=opts)\n",
    "w = np.transpose(np.array(soln.x))\n",
    "w = w[:,np.newaxis]\n",
    "rmse = testOLERegression(w,Xtrain_i,ytrain)\n",
    "print('Gradient Descent Linear Regression RMSE on train data - %.2f'%rmse)\n",
    "rmse = testOLERegression(w,Xtest_i,ytest)\n",
    "print('Gradient Descent Linear Regression RMSE on test data - %.2f'%rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 - Perceptron using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PROBLEM 3')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLinearModel(w,Xtest):\n",
    "#     print(w)\n",
    "    wtx = np.dot(Xtest,w)\n",
    "    \n",
    "    y = np.squeeze(np.asarray(wtx))\n",
    "#     print(y)\n",
    "    ypred = []\n",
    "    for i in y:\n",
    "        if(i > 0):\n",
    "            ypred.append(1)\n",
    "        else:\n",
    "            ypred.append(-1)\n",
    "#     print(ypred)\n",
    "    ypred = np.array(ypred)\n",
    "#     print(ypred)\n",
    "    ypred = ypred.reshape(-1, 1)\n",
    "#     print(ypred)\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # Xtest = N x d\n",
    "    # Output:\n",
    "    # ypred = N x 1 vector of predictions\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "#     ypred = np.zeros([Xtest.shape[0],1])\n",
    "#     print(ypred)\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateLinearModel(w,Xtest,ytest):\n",
    "    pred = predictLinearModel(w,Xtest)\n",
    "#     print(pred)\n",
    "    ytest= np.squeeze(np.asarray(ytest))\n",
    "    ypred= np.squeeze(np.asarray(pred))\n",
    "#     print(ytest)\n",
    "#     print(ypred)\n",
    "    total=len(ypred)\n",
    "#     print(total)\n",
    "    match=0\n",
    "    for i in range(0,total):\n",
    "      if ytest[i]==ypred[i]:\n",
    "        match=match+1\n",
    "#     print(match)\n",
    "    acc= match/total\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # Xtest = N x d\n",
    "    # ytest = N x 1\n",
    "    # Output:\n",
    "    # acc = scalar values\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "#     acc = 0\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Accuracy on train data - 0.84\n",
      "Perceptron Accuracy on test data - 0.84\n"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = pickle.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "args = (Xtrain_i,ytrain)\n",
    "opts = {'maxiter' : 50}    # Preferred value.    \n",
    "w_init = np.zeros((Xtrain_i.shape[1],1))\n",
    "soln = minimize(regressionObjVal, w_init, jac=regressionGradient, args=args,method='CG', options=opts)\n",
    "w = np.transpose(np.array(soln.x))\n",
    "w = w[:,np.newaxis]\n",
    "\n",
    "predictLinearModel(w,Xtrain_i)\n",
    "\n",
    "acc = evaluateLinearModel(w,Xtrain_i,ytrain)\n",
    "print('Perceptron Accuracy on train data - %.2f'%acc)\n",
    "acc = evaluateLinearModel(w,Xtest_i,ytest)\n",
    "print('Perceptron Accuracy on test data - %.2f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 - Logistic Regression Using Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PROBLEM 4')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticObjVal(w, X, y):\n",
    "    n = len(X)\n",
    "    w = np.matrix(w)\n",
    "#     print(X[0])\n",
    "    sum = 0\n",
    "    for i in range(0,n):\n",
    "        wtx = np.dot(w,X[i])\n",
    "#         y = y[i][:,np.newaxis]\n",
    "        ywtx = np.dot(-y[i],wtx)\n",
    "#         print(ywtx)\n",
    "#         ywtx = np.asarray(ywtx)\n",
    "        ywtx = np.squeeze(np.asarray(ywtx))\n",
    "#         print(ywtx)\n",
    "#         print(np.matrix(y[i]))\n",
    "        expt = math.exp(ywtx)\n",
    "    \n",
    "        logt = math.log(1+expt)\n",
    "        sum = sum+logt\n",
    "#         print(logt)\n",
    "#         print(\"=======\")\n",
    "    # compute log-loss error (scalar) with respect\n",
    "    # to w (vector) for the given data X and y                               \n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # error = scalar\n",
    "#     print(sum)\n",
    "    error = sum/n\n",
    "    print(error)\n",
    "    if len(w.shape) == 1:\n",
    "        w = w[:,np.newaxis]\n",
    "#     print(w)\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "#     error = 0\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticGradient(w, X, y):\n",
    "    n = len(X)\n",
    "    w = np.transpose(w)\n",
    "    \n",
    "#     print(X[0])\n",
    "    y= np.squeeze(np.asarray(y))\n",
    "    sum = 0\n",
    "    for i in range(0,n):\n",
    "        wtx = np.dot(w,X[i])\n",
    "        ywtx = np.dot(y[i],wtx)\n",
    "        expt = 1 + math.exp(ywtx) \n",
    "        expt = np.squeeze(np.asarray(expt))\n",
    "        yi = y[i] / expt\n",
    "        num = np.dot(yi,X[i])\n",
    "        sum = sum +num\n",
    "    # compute the gradient of the log-loss error (vector) with respect\n",
    "    # to w (vector) for the given data X and y  \n",
    "    #\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # error = d length gradient vector (not a d x 1 matrix)\n",
    "    gradient = sum*(-1/n)\n",
    "    if len(w.shape) == 1:\n",
    "        w = w[:,np.newaxis]\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    gradient = np.zeros((w.shape[0],))\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-45-327a9a26c45b>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-45-327a9a26c45b>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    for i in range(0,n):\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def logisticHessian(w, X, y):\n",
    "    n = len(X)\n",
    "    w = np.matrix(w)\n",
    "#     print(X[0])\n",
    "    sum = np.zeros((len(X[0]),len(X[0]))\n",
    "    for i in range(0,n):\n",
    "        wtx = np.dot(w,X[i])\n",
    "        ywtx = np.dot(y[i],wtx)\n",
    "        exp =  math.exp(ywtx) \n",
    "        xxt = np.dot(X[i],np.transpose(X[i]))\n",
    "        sum = np.add(sum,exp/(1+exp)*xxt)\n",
    "        \n",
    "    # compute the Hessian of the log-loss error (matrix) with respect\n",
    "    # to w (vector) for the given data X and y                               \n",
    "    #\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # Hessian = d x d matrix\n",
    "    \n",
    "    if len(w.shape) == 1:\n",
    "        w = w[:,np.newaxis]\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    hessian = np.eye(X.shape[1])\n",
    "    print(hessian)\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599458\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,) and (3,) not aligned: 1 (dim 0) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-b9b85bf438fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'maxiter'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m}\u001b[0m    \u001b[1;31m# Preferred value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mw_init\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0msoln\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogisticObjVal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogisticGradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogisticHessian\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Newton-CG'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoln\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'newton-cg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m--> 607\u001b[1;33m                                   **options)\n\u001b[0m\u001b[0;32m    608\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_minimize_newtoncg\u001b[1;34m(fun, x0, args, jac, hess, hessp, callback, xtol, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[0;32m   1595\u001b[0m         \u001b[1;31m# Compute a search direction pk by applying the CG method to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1596\u001b[0m         \u001b[1;31m#  del2 f(xk) p = - grad f(xk) starting from 0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1597\u001b[1;33m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mfprime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1598\u001b[0m         \u001b[0mmaggrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1599\u001b[0m         \u001b[0meta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaggrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-ecf11ac32073>\u001b[0m in \u001b[0;36mlogisticGradient\u001b[1;34m(w, X, y)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mexpt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mywtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0myi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mexpt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0msum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m# compute the gradient of the log-loss error (vector) with respect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# to w (vector) for the given data X and y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,) and (3,) not aligned: 1 (dim 0) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = pickle.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "args = (Xtrain_i,ytrain)\n",
    "opts = {'maxiter' : 50}    # Preferred value.    \n",
    "w_init = np.zeros((Xtrain_i.shape[1],1))\n",
    "soln = minimize(logisticObjVal, w_init, jac=logisticGradient, hess=logisticHessian, args=args,method='Newton-CG', options=opts)\n",
    "w = np.transpose(np.array(soln.x))\n",
    "w = np.reshape(w,[len(w),1])\n",
    "acc = evaluateLinearModel(w,Xtrain_i,ytrain)\n",
    "print('Logistic Regression Accuracy on train data - %.2f'%acc)\n",
    "acc = evaluateLinearModel(w,Xtest_i,ytest)\n",
    "print('Logistic Regression Accuracy on test data - %.2f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 - Support Vector Machines Using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PROBLEM 5')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSGDSVM(X,y,T,eta=0.01):\n",
    "    # learn a linear SVM by implementing the SGD algorithm\n",
    "    #\n",
    "    w = np.zeros(len(X))\n",
    "    for j in range(T):\n",
    "        i = random.randint(1,len(X))\n",
    "        ywt = np.dot(y[i],np.transpose(w))\n",
    "        ywtxi = np.dot(ywt,X[i])\n",
    "        if(ywtxi<1):\n",
    "            yixi = np.dot(y[i],X[i])\n",
    "            w = w-eta*yixi\n",
    "    # Inputs:\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # T = number of iterations\n",
    "    # eta = learning rate\n",
    "    # Output:\n",
    "    # weight vector, w = d x 1\n",
    "    \n",
    "    # IMPLEMENT THIS METHOD\n",
    "    w = np.zeros([X.shape[1],1])\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = pickle.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "args = (Xtrain_i,ytrain)\n",
    "w = trainSGDSVM(Xtrain_i,ytrain,100,0.01)\n",
    "acc = evaluateLinearModel(w,Xtrain_i,ytrain)\n",
    "print('SVM Accuracy on train data - %.2f'%acc)\n",
    "acc = evaluateLinearModel(w,Xtest_i,ytest)\n",
    "print('SVM Accuracy on test data - %.2f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6 - Plotting decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Problem 6')\n",
    "print('---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBoundaries(w,X,y):\n",
    "    # plotting boundaries\n",
    "\n",
    "    mn = np.min(X,axis=0)\n",
    "    mx = np.max(X,axis=0)\n",
    "    x1 = np.linspace(mn[1],mx[1],100)\n",
    "    x2 = np.linspace(mn[2],mx[2],100)\n",
    "    xx1,xx2 = np.meshgrid(x1,x2)\n",
    "    xx = np.zeros((x1.shape[0]*x2.shape[0],2))\n",
    "    xx[:,0] = xx1.ravel()\n",
    "    xx[:,1] = xx2.ravel()\n",
    "    xx_i = np.concatenate((np.ones((xx.shape[0],1)), xx), axis=1)\n",
    "    ypred = predictLinearModel(w,xx_i)\n",
    "    ax.contourf(x1,x2,ypred.reshape((x1.shape[0],x2.shape[0])),alpha=0.3,cmap='cool')\n",
    "    ax.scatter(X[:,1],X[:,2],c=y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = pickle.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "# Replace next three lines with code for learning w using the three methods\n",
    "w_perceptron = np.zeros((Xtrain_i.shape[1],1))\n",
    "w_logistic = np.zeros((Xtrain_i.shape[1],1))\n",
    "w_svm = np.zeros((Xtrain_i.shape[1],1))\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "\n",
    "ax = plt.subplot(1,3,1)\n",
    "plotBoundaries(w_perceptron,Xtrain_i,ytrain)\n",
    "ax.set_title('Perceptron')\n",
    "\n",
    "ax = plt.subplot(1,3,2)\n",
    "plotBoundaries(w_logistic,Xtrain_i,ytrain)\n",
    "ax.set_title('Logistic Regression')\n",
    "\n",
    "ax = plt.subplot(1,3,3)\n",
    "plotBoundaries(w_svm,Xtrain_i,ytrain)\n",
    "ax.set_title('SVM')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
