{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE474/574 - Programming Assignment 1\n",
    "\n",
    "For grading, we will execute the submitted notebook as follows:\n",
    "\n",
    "```shell\n",
    "jupyter nbconvert --to python PA1Script.ipynb\n",
    "python PA1Script.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 - Linear Regression with Direct Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM 1\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('PROBLEM 1')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnOLERegression(Xtrain,ytrain):\n",
    "   print(Xtrain)\n",
    "   print(\"------------\")\n",
    "   A=np.array(Xtrain)\n",
    "   print(A)\n",
    "   B=np.array(ytrain)\n",
    "   At =np.transpose(A)\n",
    "   AtA=np.dot(At,A)\n",
    "   AtA_Inverse=np.linalg.inv(AtA)\n",
    "   AtA_InverseAt=np.dot(np.linalg.inv(AtA),At)\n",
    "   w=np.dot(AtA_InverseAt,B)\n",
    "   return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOLERegression(w,Xtest,ytest):\n",
    "    A=np.array(Xtest)\n",
    "    B=np.array(ytest)\n",
    "    wt=np.transpose(w)\n",
    "    sum=0\n",
    "#     print(len(A[0]))\n",
    "    for i in range(0,np.size(A[1])):\n",
    "     wtxi=np.dot(wt,A[i])\n",
    "     yiwtxi=B[i]-wtxi\n",
    "     yiwtxi2=yiwtxi**2\n",
    "     sum=sum+yiwtxi2\n",
    "    average=sum/np.size(A)\n",
    "    rmse=math.sqrt(average)\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03807591  0.05068012  0.06169621 ... -0.02819118 -0.01765816\n",
      "  -0.02779368]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ...  0.02529772  0.05303354\n",
      "   0.10401328]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.02733183 -0.01723596\n",
      "  -0.02230374]\n",
      " ...\n",
      " [ 0.02354575 -0.04464164  0.01966154 ... -0.03051743 -0.02103573\n",
      "   0.04281168]\n",
      " [ 0.04897352  0.05068012  0.07462995 ... -0.0047289  -0.00531486\n",
      "  -0.00773833]\n",
      " [ 0.03081083  0.05068012 -0.00836158 ... -0.02251995 -0.02765115\n",
      "  -0.05413222]]\n",
      "[[ 0.03807591  0.05068012  0.06169621 ... -0.02819118 -0.01765816\n",
      "  -0.02779368]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ...  0.02529772  0.05303354\n",
      "   0.10401328]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.02733183 -0.01723596\n",
      "  -0.02230374]\n",
      " ...\n",
      " [ 0.02354575 -0.04464164  0.01966154 ... -0.03051743 -0.02103573\n",
      "   0.04281168]\n",
      " [ 0.04897352  0.05068012  0.07462995 ... -0.0047289  -0.00531486\n",
      "  -0.00773833]\n",
      " [ 0.03081083  0.05068012 -0.00836158 ... -0.02251995 -0.02765115\n",
      "  -0.05413222]]\n",
      "65\n",
      "RMSE without intercept on train data - 8.58\n",
      "RMSE with intercept on train data - 3.00\n",
      "RMSE without intercept on test data - 18.02\n",
      "RMSE with intercept on test data - 4.80\n"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain,Xtest,ytest = pickle.load(open('diabetes.pickle','rb'),encoding='latin1')   \n",
    "# add intercept\n",
    "x1 = np.ones((len(Xtrain),1))\n",
    "x2 = np.ones((len(Xtest),1))\n",
    "\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "w = learnOLERegression(Xtrain,ytrain)\n",
    "w_i = learnOLERegression2(Xtrain_i,ytrain)\n",
    "print(np.size(w_i))\n",
    "rmse = testOLERegression(w,Xtrain,ytrain)\n",
    "\n",
    "rmse_i = testOLERegression(w_i,Xtrain_i,ytrain)\n",
    "print('RMSE without intercept on train data - %.2f'%rmse)\n",
    "print('RMSE with intercept on train data - %.2f'%rmse_i)\n",
    "\n",
    "rmse = testOLERegression(w,Xtest,ytest)\n",
    "rmse_i = testOLERegression(w_i,Xtest_i,ytest)\n",
    "print('RMSE without intercept on test data - %.2f'%rmse)\n",
    "print('RMSE with intercept on test data - %.2f'%rmse_i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 - Linear Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PROBLEM 2')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionObjVal(w, X, y):\n",
    "    w=np.matrix(w)\n",
    "    w=w.transpose()\n",
    "    A=np.dot(X,w)\n",
    "    B=np.subtract(y,A)\n",
    "    Bt=B.transpose()\n",
    "    e=np.dot(Bt,B)\n",
    "    E=0.5*e\n",
    "    error=E[0,0]\n",
    "#     print(error)\n",
    "    # compute squared error (scalar) with respect\n",
    "    # to w (vector) for the given data X and y      \n",
    "    #\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # error = scalar value\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    #error = 0\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionGradient(w, X, y):\n",
    "    AT = X.transpose()\n",
    "    ATA = np.dot(AT,X)\n",
    "#     print(ATA)\n",
    "#     print('=============')\n",
    "    w=np.matrix(w)\n",
    "    w=w.transpose()\n",
    "    ATAW = np.dot(ATA,w)\n",
    "#     print(ATAW)\n",
    "    ATB = np.dot(AT,y)\n",
    "    deltaJ = np.subtract(ATAW,ATB)\n",
    "#     print(deltaJ[0])\n",
    "    \n",
    "    error_grad = np.squeeze(np.asarray(deltaJ))\n",
    "#     print(error_grad)\n",
    "    # compute gradient of squared error (scalar) with respect\n",
    "    # to w (vector) for the given data X and y   \n",
    "    \n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # gradient = d length vector (not a d x 1 matrix)\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE \n",
    "\n",
    "#     error_grad = np.zeros((X.shape[1],))\n",
    "#     print(error_grad)\n",
    "    return error_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Linear Regression RMSE on train data - 3.07\n",
      "Gradient Descent Linear Regression RMSE on test data - 4.47\n"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain,Xtest,ytest = pickle.load(open('diabetes.pickle','rb'),encoding='latin1')   \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "args = (Xtrain_i,ytrain)\n",
    "opts = {'maxiter' : 50}    # Preferred value.    \n",
    "w_init = np.zeros((Xtrain_i.shape[1],1))\n",
    "soln = minimize(regressionObjVal, w_init, jac=regressionGradient, args=args,method='CG', options=opts)\n",
    "w = np.transpose(np.array(soln.x))\n",
    "w = w[:,np.newaxis]\n",
    "rmse = testOLERegression(w,Xtrain_i,ytrain)\n",
    "print('Gradient Descent Linear Regression RMSE on train data - %.2f'%rmse)\n",
    "rmse = testOLERegression(w,Xtest_i,ytest)\n",
    "print('Gradient Descent Linear Regression RMSE on test data - %.2f'%rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 - Perceptron using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PROBLEM 3')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "# def regressionObjVal(w, X, y):\n",
    "#   w=np.matrix(w)\n",
    "#   w=w.transpose()\n",
    "#   A=np.dot(X,w)\n",
    "#   B=np.subtract(y,A)\n",
    "#   Bt=B.transpose()\n",
    "#   e=np.dot(Bt,B)\n",
    "#   E=0.5*e\n",
    "#   error=E[0,0]\n",
    "#   return error\n",
    "\n",
    "# def regressionGradient(w, X, y):\n",
    "#   w=np.matrix(w)\n",
    "#   w=w.transpose()\n",
    "#   A=X.transpose()\n",
    "#   B=X\n",
    "#   B1=np.dot(A,B)\n",
    "#   B2=np.dot(B1,w)\n",
    "#   C=np.dot(A,y)\n",
    "#   D=B2-C\n",
    "#   D= np.squeeze(np.asarray(D))\n",
    "#   return D  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLinearModel(w,Xtest):\n",
    "    print(w)\n",
    "    wtx = np.dot(Xtest,w)\n",
    "    \n",
    "    y = np.squeeze(np.asarray(wtx))\n",
    "    print(y)\n",
    "    ypred = []\n",
    "    for i in y:\n",
    "        if(i > 0):\n",
    "            ypred.append(1)\n",
    "        else:\n",
    "            ypred.append(-1)\n",
    "    print(ypred)\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # Xtest = N x d\n",
    "    # Output:\n",
    "    # ypred = N x 1 vector of predictions\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    ypred = np.zeros([Xtest.shape[0],1])\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateLinearModel(w,Xtest,ytest):\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # Xtest = N x d\n",
    "    # ytest = N x 1\n",
    "    # Output:\n",
    "    # acc = scalar values\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    acc = 0\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0881355 ]\n",
      " [ 0.28506787]\n",
      " [-0.44939862]]\n",
      "[ 1.16850571  0.71176206  0.98410135 -0.54911059 -0.13181726 -0.41341988\n",
      " -0.05221272  0.16976168 -0.01645664 -0.18618108 -0.25704278  1.00102619\n",
      " -0.79012519 -0.71365544 -0.05566753  0.52563323  0.56352636  1.04666937\n",
      "  0.04938761 -0.18795813 -0.16691849  0.73758744 -1.21170854  0.29171098\n",
      "  1.02979469  0.65397903  0.33050126 -0.48668059  0.76919735 -0.6944933\n",
      "  1.35522122  1.44057828  1.47024221 -0.73117633 -0.08867003  1.56046918\n",
      " -0.66804667  0.57730486 -0.0058352   0.11870536  0.2398941   0.68116452\n",
      " -0.34205676  0.03697825 -0.34835979 -1.38626791 -0.60365128 -0.40042824\n",
      "  0.35510575 -1.11172911  1.49055186  0.19385935  1.36901134  0.19555698\n",
      " -0.80580697  0.06696253  0.08375397  0.17205729 -0.45772662 -0.79024485\n",
      " -0.16742125  0.01226365 -0.59165793 -0.47955102 -0.49259958 -0.62980498\n",
      "  0.34482687  0.80830885 -1.15268063 -0.16495998  0.04017991  1.13726473\n",
      "  0.30005169  0.74066528  0.09547864 -0.5805698  -0.36092859  0.49011372\n",
      " -0.87578922  1.51226501  1.06587997 -0.1551167  -0.42021774  0.07361545\n",
      " -0.19837352  0.33501715  0.33455798  0.25024923  0.8209529  -0.09676692\n",
      "  1.05203249  0.26084035 -0.89296668 -0.15028072 -0.96931024  0.69297393\n",
      "  0.12201861 -1.01522185 -0.3225766  -0.55987596]\n",
      "[1.1685057051345327, 0.7117620649156674, 0.9841013509733743, 0.1697616757292418, 1.0010261899938868, 0.5256332263364524, 0.563526357017302, 1.0466693741142041, 0.04938760786779012, 0.7375874357808608, 0.2917109826248636, 1.0297946925341324, 0.6539790267974217, 0.33050126079899744, 0.7691973495707722, 1.3552212247771158, 1.4405782813831347, 1.4702422145908869, 1.5604691841640623, 0.5773048639362853, 0.11870536441369961, 0.23989409518034677, 0.6811645210026291, 0.036978251840393314, 0.3551057540219591, 1.4905518619118125, 0.19385935267857646, 1.36901134369272, 0.19555698480719214, 0.06696252941246944, 0.08375396553466669, 0.17205728847851132, 0.012263654909105637, 0.3448268686026836, 0.8083088516978354, 0.04017991087691961, 1.1372647308469788, 0.30005169181983854, 0.7406652832118003, 0.09547863559454695, 0.49011371831047557, 1.5122650078785445, 1.065879974176279, 0.07361544688906374, 0.33501714797908333, 0.33455798207765175, 0.250249232792745, 0.8209529011397466, 1.0520324932242975, 0.2608403497000946, 0.692973932078667, 0.1220186140272248]\n",
      "Perceptron Accuracy on train data - 0.00\n",
      "Perceptron Accuracy on test data - 0.00\n"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = pickle.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "args = (Xtrain_i,ytrain)\n",
    "opts = {'maxiter' : 50}    # Preferred value.    \n",
    "w_init = np.zeros((Xtrain_i.shape[1],1))\n",
    "soln = minimize(regressionObjVal, w_init, jac=regressionGradient, args=args,method='CG', options=opts)\n",
    "w = np.transpose(np.array(soln.x))\n",
    "w = w[:,np.newaxis]\n",
    "\n",
    "predictLinearModel(w,Xtrain_i)\n",
    "\n",
    "acc = evaluateLinearModel(w,Xtrain_i,ytrain)\n",
    "print('Perceptron Accuracy on train data - %.2f'%acc)\n",
    "acc = evaluateLinearModel(w,Xtest_i,ytest)\n",
    "print('Perceptron Accuracy on test data - %.2f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 - Logistic Regression Using Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PROBLEM 4')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticObjVal(w, X, y):\n",
    "\n",
    "    # compute log-loss error (scalar) with respect\n",
    "    # to w (vector) for the given data X and y                               \n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # error = scalar\n",
    "    \n",
    "    \n",
    "    if len(w.shape) == 1:\n",
    "        w = w[:,np.newaxis]\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    error = 0\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticGradient(w, X, y):\n",
    "\n",
    "    # compute the gradient of the log-loss error (vector) with respect\n",
    "    # to w (vector) for the given data X and y  \n",
    "    #\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # error = d length gradient vector (not a d x 1 matrix)\n",
    "\n",
    "    if len(w.shape) == 1:\n",
    "        w = w[:,np.newaxis]\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    gradient = np.zeros((w.shape[0],))\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticHessian(w, X, y):\n",
    "\n",
    "    # compute the Hessian of the log-loss error (matrix) with respect\n",
    "    # to w (vector) for the given data X and y                               \n",
    "    #\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # Hessian = d x d matrix\n",
    "    \n",
    "    if len(w.shape) == 1:\n",
    "        w = w[:,np.newaxis]\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    hessian = np.eye(X.shape[1])\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = pickle.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "args = (Xtrain_i,ytrain)\n",
    "opts = {'maxiter' : 50}    # Preferred value.    \n",
    "w_init = np.zeros((Xtrain_i.shape[1],1))\n",
    "soln = minimize(logisticObjVal, w_init, jac=logisticGradient, hess=logisticHessian, args=args,method='Newton-CG', options=opts)\n",
    "w = np.transpose(np.array(soln.x))\n",
    "w = np.reshape(w,[len(w),1])\n",
    "acc = evaluateLinearModel(w,Xtrain_i,ytrain)\n",
    "print('Logistic Regression Accuracy on train data - %.2f'%acc)\n",
    "acc = evaluateLinearModel(w,Xtest_i,ytest)\n",
    "print('Logistic Regression Accuracy on test data - %.2f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 - Support Vector Machines Using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PROBLEM 5')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSGDSVM(X,y,T,eta=0.01):\n",
    "    # learn a linear SVM by implementing the SGD algorithm\n",
    "    #\n",
    "    # Inputs:\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # T = number of iterations\n",
    "    # eta = learning rate\n",
    "    # Output:\n",
    "    # weight vector, w = d x 1\n",
    "    \n",
    "    # IMPLEMENT THIS METHOD\n",
    "    w = np.zeros([X.shape[1],1])\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = pickle.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "args = (Xtrain_i,ytrain)\n",
    "w = trainSGDSVM(Xtrain_i,ytrain,100,0.01)\n",
    "acc = evaluateLinearModel(w,Xtrain_i,ytrain)\n",
    "print('SVM Accuracy on train data - %.2f'%acc)\n",
    "acc = evaluateLinearModel(w,Xtest_i,ytest)\n",
    "print('SVM Accuracy on test data - %.2f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6 - Plotting decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Problem 6')\n",
    "print('---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBoundaries(w,X,y):\n",
    "    # plotting boundaries\n",
    "\n",
    "    mn = np.min(X,axis=0)\n",
    "    mx = np.max(X,axis=0)\n",
    "    x1 = np.linspace(mn[1],mx[1],100)\n",
    "    x2 = np.linspace(mn[2],mx[2],100)\n",
    "    xx1,xx2 = np.meshgrid(x1,x2)\n",
    "    xx = np.zeros((x1.shape[0]*x2.shape[0],2))\n",
    "    xx[:,0] = xx1.ravel()\n",
    "    xx[:,1] = xx2.ravel()\n",
    "    xx_i = np.concatenate((np.ones((xx.shape[0],1)), xx), axis=1)\n",
    "    ypred = predictLinearModel(w,xx_i)\n",
    "    ax.contourf(x1,x2,ypred.reshape((x1.shape[0],x2.shape[0])),alpha=0.3,cmap='cool')\n",
    "    ax.scatter(X[:,1],X[:,2],c=y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = pickle.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "# Replace next three lines with code for learning w using the three methods\n",
    "w_perceptron = np.zeros((Xtrain_i.shape[1],1))\n",
    "w_logistic = np.zeros((Xtrain_i.shape[1],1))\n",
    "w_svm = np.zeros((Xtrain_i.shape[1],1))\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "\n",
    "ax = plt.subplot(1,3,1)\n",
    "plotBoundaries(w_perceptron,Xtrain_i,ytrain)\n",
    "ax.set_title('Perceptron')\n",
    "\n",
    "ax = plt.subplot(1,3,2)\n",
    "plotBoundaries(w_logistic,Xtrain_i,ytrain)\n",
    "ax.set_title('Logistic Regression')\n",
    "\n",
    "ax = plt.subplot(1,3,3)\n",
    "plotBoundaries(w_svm,Xtrain_i,ytrain)\n",
    "ax.set_title('SVM')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
